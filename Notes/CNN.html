<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=XGMkxXUZTA64h2imyzu79g);ul.lst-kix_suckqeh6s0ik-8{list-style-type:none}ul.lst-kix_suckqeh6s0ik-6{list-style-type:none}.lst-kix_1fvubqgwzk0d-2>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-2}ul.lst-kix_suckqeh6s0ik-7{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-2.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-2 0}.lst-kix_1fvubqgwzk0d-8>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-8}.lst-kix_1fvubqgwzk0d-3>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-3}.lst-kix_suckqeh6s0ik-4>li:before{content:"-  "}.lst-kix_suckqeh6s0ik-2>li:before{content:"-  "}.lst-kix_suckqeh6s0ik-3>li:before{content:"-  "}ul.lst-kix_suckqeh6s0ik-4{list-style-type:none}.lst-kix_suckqeh6s0ik-1>li:before{content:"-  "}ul.lst-kix_suckqeh6s0ik-5{list-style-type:none}.lst-kix_suckqeh6s0ik-0>li:before{content:"-  "}ul.lst-kix_suckqeh6s0ik-2{list-style-type:none}ul.lst-kix_suckqeh6s0ik-3{list-style-type:none}ul.lst-kix_suckqeh6s0ik-0{list-style-type:none}ul.lst-kix_suckqeh6s0ik-1{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-6.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-6 0}.lst-kix_suckqeh6s0ik-5>li:before{content:"-  "}.lst-kix_1fvubqgwzk0d-0>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-0}.lst-kix_suckqeh6s0ik-6>li:before{content:"-  "}.lst-kix_suckqeh6s0ik-7>li:before{content:"-  "}.lst-kix_suckqeh6s0ik-8>li:before{content:"-  "}ol.lst-kix_1fvubqgwzk0d-3.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-3 0}ol.lst-kix_1fvubqgwzk0d-7.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-7 0}.lst-kix_1fvubqgwzk0d-5>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-5}ol.lst-kix_1fvubqgwzk0d-0.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-0 0}.lst-kix_1fvubqgwzk0d-7>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-7,lower-latin) ". "}.lst-kix_1fvubqgwzk0d-6>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-6,decimal) ". "}.lst-kix_1fvubqgwzk0d-5>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-5,lower-roman) ". "}ol.lst-kix_1fvubqgwzk0d-4.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-4 0}ol.lst-kix_1fvubqgwzk0d-8{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-7{list-style-type:none}.lst-kix_1fvubqgwzk0d-6>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-6}ol.lst-kix_1fvubqgwzk0d-0{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-2{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-1{list-style-type:none}.lst-kix_1fvubqgwzk0d-8>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-8,lower-roman) ". "}ol.lst-kix_1fvubqgwzk0d-4{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-3{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-6{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-5{list-style-type:none}ol.lst-kix_1fvubqgwzk0d-1.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-1 0}.lst-kix_1fvubqgwzk0d-1>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-1}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_1fvubqgwzk0d-2>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-2,lower-roman) ". "}.lst-kix_1fvubqgwzk0d-4>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-4,lower-latin) ". "}ol.lst-kix_1fvubqgwzk0d-5.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-5 0}.lst-kix_1fvubqgwzk0d-3>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-3,decimal) ". "}.lst-kix_1fvubqgwzk0d-7>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-7}.lst-kix_1fvubqgwzk0d-0>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-0,decimal) ". "}.lst-kix_1fvubqgwzk0d-4>li{counter-increment:lst-ctn-kix_1fvubqgwzk0d-4}.lst-kix_1fvubqgwzk0d-1>li:before{content:"" counter(lst-ctn-kix_1fvubqgwzk0d-1,lower-latin) ". "}ol.lst-kix_1fvubqgwzk0d-8.start{counter-reset:lst-ctn-kix_1fvubqgwzk0d-8 0}ol{margin:0;padding:0}table td,table th{padding:0}.c0{border-right-style:solid;padding-top:0pt;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.7;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;orphans:2;widows:2;text-align:left;padding-right:0pt}.c27{border-right-style:solid;padding-top:0pt;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.4;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;orphans:2;widows:2;text-align:left;padding-right:0pt}.c39{border-right-style:solid;border-top-width:0pt;border-right-width:0pt;border-top-style:solid;border-bottom-width:0pt;border-bottom-style:solid;padding-right:0pt}.c21{-webkit-text-decoration-skip:none;color:#008abc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:15pt;font-family:"Arial";font-style:normal}.c3{color:#3c4043;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c9{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.7;orphans:2;widows:2;text-align:left;height:11pt}.c36{padding-top:0pt;padding-bottom:0pt;line-height:1.7;orphans:2;widows:2;text-align:left}.c43{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c18{-webkit-text-decoration-skip:none;color:#008abc;text-decoration:underline;text-decoration-skip-ink:none;font-size:10.5pt}.c5{background-color:#ffffff;font-size:10.5pt;font-family:"Roboto Mono";color:#3c4043;font-weight:400}.c31{background-color:#f1f3f4;font-size:10.5pt;font-family:"Roboto Mono";color:#666666;font-weight:400}.c17{background-color:#f1f3f4;font-size:10.5pt;font-family:"Roboto Mono";color:#008abc;font-weight:400}.c10{background-color:#f1f3f4;font-size:10.5pt;font-family:"Roboto Mono";color:#3c4043;font-weight:400}.c28{-webkit-text-decoration-skip:none;color:#008abc;text-decoration:underline;text-decoration-skip-ink:none;font-size:18pt}.c25{background-color:#f1f3f4;font-size:10.5pt;font-family:"Roboto Mono";color:#bb2323;font-weight:400}.c29{color:#202214;font-weight:400;font-size:15pt;font-family:"Arial"}.c37{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c4{background-color:#ffffff;font-size:10.5pt;font-style:italic;color:#3c4043}.c24{font-size:10.5pt;font-family:"Roboto Mono";color:#188038;font-weight:400}.c41{font-weight:400;vertical-align:baseline;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;font-size:11pt;font-family:"Arial"}.c22{color:#202214;font-weight:400;font-size:18pt;font-family:"Arial"}.c7{background-color:#ffffff;font-size:10.5pt;color:#3c4043}.c30{color:#3c4043;font-size:23pt;font-family:"Arial"}.c33{background-color:#e6b8af;font-family:"Roboto Mono";font-weight:400}.c40{color:#3c4043;font-size:11.5pt;font-style:normal}.c38{color:#000000;font-size:11pt;font-style:normal}.c11{text-decoration:none;vertical-align:baseline;font-style:normal}.c34{font-size:18pt;color:#202214}.c46{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c26{margin-left:36pt;padding-left:0pt}.c12{color:#3c4043;font-size:10.5pt}.c35{font-family:"Roboto Mono";font-weight:400}.c6{color:inherit;text-decoration:inherit}.c20{padding:0;margin:0}.c19{font-weight:700}.c44{font-style:normal}.c23{font-style:italic}.c42{page-break-after:avoid}.c32{background-color:#d9ead3}.c15{background-color:#d9d2e9}.c16{background-color:#ffffff}.c13{height:11pt}.c45{margin-left:6pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{border-right-style:solid;padding-top:0pt;color:#202214;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;font-size:15pt;padding-bottom:0pt;line-height:1.4;page-break-after:avoid;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;font-family:"Arial";border-bottom-style:solid;orphans:2;widows:2;text-align:left;padding-right:0pt}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c16 c46 doc-content"><p class="c0"><span class="c12">A convnet used for image classification consists of two parts: a </span><span class="c12 c19">convolutional base</span><span class="c12">&nbsp;and a </span><span class="c12 c19">dense head</span><span class="c3">.</span></p><p class="c2"><span class="c14 c11"></span></p><h1 class="c27" id="h.41erd78kz9xh"><span class="c11 c22">The Convolutional Classifier</span></h1><p class="c0"><span class="c12">A convnet used for image classification consists of two parts: a </span><span class="c12 c19">convolutional base</span><span class="c12">&nbsp;and a </span><span class="c12 c19">dense head</span><span class="c3">.</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 210.67px;"><img alt="The parts of a convnet: image, base, head, class; input, extract, classify, output." src="images/image7.png" style="width: 624.00px; height: 210.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c12">The base is used to </span><span class="c12 c19">extract the features</span><span class="c3">&nbsp;from an image. It is formed primarily of layers performing the convolution operation, but often includes other kinds of layers as well. (You&#39;ll learn about these in the next lesson.)</span></p><p class="c0"><span class="c12">The head is used to </span><span class="c12 c19">determine the class</span><span class="c3">&nbsp;of the image. It is formed primarily of dense layers, but might include other layers like dropout.</span></p><h1 class="c27" id="h.quryonglv0b"><span class="c34">Training the Classifier</span><span class="c28"><a class="c6" href="https://www.google.com/url?q=https://www.kaggle.com/code/ryanholbrook/the-convolutional-classifier%23Training-the-Classifier&amp;sa=D&amp;source=editors&amp;ust=1727506785326284&amp;usg=AOvVaw10fi7t6Y39hUFDvLSglBrr">&para;</a></span></h1><p class="c0"><span class="c3">The goal of the network during training is to learn two things:</span></p><ol class="c20 lst-kix_1fvubqgwzk0d-0 start" start="1"><li class="c8 c26 c39 li-bullet-0"><span class="c3">which features to extract from an image (base),</span></li><li class="c39 c8 c26 li-bullet-0"><span class="c3">which class goes with what features (head).</span></li></ol><p class="c0"><span class="c12">These days, convnets are rarely trained from scratch. More often, we </span><span class="c12 c19">reuse the base of a pretrained model</span><span class="c12">. To the pretrained base we then </span><span class="c12 c19">attach an untrained head</span><span class="c12">. In other words, we reuse the part of a network that has already learned to do </span><span class="c12 c23">1. Extract features</span><span class="c3">, and attach to it some fresh layers to learn </span></p><p class="c8"><span class="c7">Reusing a pretrained model is a technique known as </span><span class="c7 c19">transfer learning</span><span class="c7">. It is so effective, that almost every image classifier these days will make use of it.</span></p><p class="c2"><span class="c14 c11"></span></p><p class="c2"><span class="c14 c11"></span></p><p class="c8"><span class="c14 c11">CNN layers see last layer as batches, </span></p><ul class="c20 lst-kix_suckqeh6s0ik-0 start"><li class="c8 c26 li-bullet-0"><span class="c14 c11">Producing feature maps CNN : We can use many different filters to detect specific attributes of pictures </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 566.50px; height: 354.97px;"><img alt="" src="images/image3.png" style="width: 566.50px; height: 354.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><p class="c2"><span class="c14 c11"></span></p><p class="c2"><span class="c14 c11"></span></p><h2 class="c27" id="h.5vz3va98269c"><span>Weights</span><span><a class="c6" href="https://www.google.com/url?q=https://www.kaggle.com/code/ryanholbrook/convolution-and-relu%23Weights&amp;sa=D&amp;source=editors&amp;ust=1727506785327467&amp;usg=AOvVaw0O7mygYF0pX9K9rSMXlEeo">&para;</a></span></h2><p class="c0"><span class="c12">The </span><span class="c12 c19">weights</span><span class="c12">&nbsp;a convnet learns during training are primarily contained in its convolutional layers. These weights we call </span><span class="c12 c19">kernels</span><span class="c3">. We can represent them as small arrays:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 102.30px; height: 102.30px;"><img alt="A 3x3 kernel." src="images/image8.png" style="width: 102.30px; height: 102.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c13"><span class="c3"></span></p><p class="c0"><span class="c12">A kernel operates by scanning over an image and producing a </span><span class="c12 c23">weighted sum</span><span class="c3">&nbsp;of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 188.80px; height: 201.50px;"><img alt="A kernel acts as a kind of lens." src="images/image2.png" style="width: 188.80px; height: 201.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c7">A kernel operates by scanning over an image and producing a </span><span class="c4">weighted sum</span><span class="c3 c16">&nbsp;of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.</span></p><p class="c2"><span class="c3 c16"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 269.33px;"><img alt="Three kernels and the feature maps they produce." src="images/image9.png" style="width: 624.00px; height: 269.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c43"><span class="c4 c37">Kernels and features.</span></p><p class="c0"><span class="c7">From the pattern of numbers in the kernel, you can tell the kinds of feature maps it creates. </span><span class="c12 c32">Generally, what a convolution accentuates in its inputs will match the shape of the </span><span class="c12 c23 c32">positive</span><span class="c12 c32">&nbsp;numbers in the kernel. The left and middle kernels above will both filter for horizontal shapes. </span><span class="c7">With the </span><span class="c12 c33">filters</span><span class="c7">&nbsp;parameter, you tell the convolutional layer</span><span class="c3 c15">&nbsp;how many feature maps you want it to create as output.</span></p><p class="c0 c13"><span class="c3 c15"></span></p><h2 class="c0 c42" id="h.t2fwegg1637g"><span class="c11 c29">Detect with ReLU</span></h2><p class="c8"><span class="c3 c16">You could think about the activation function as scoring pixel values according to some measure of importance. The ReLU activation says that negative values are not important and so sets them to 0. (&quot;Everything unimportant is equally unimportant.&quot;)</span></p><p class="c2"><span class="c3 c16"></span></p><p class="c2"><span class="c3 c16"></span></p><p class="c2"><span class="c3 c16"></span></p><h2 class="c27 c42" id="h.b0ojr4if3s72"><span class="c29 c11">MAXPool</span></h2><p class="c8"><span class="c7">A </span><span class="c5">MaxPool2D</span><span class="c7">&nbsp;layer is much like a </span><span class="c5">Conv2D</span><span class="c7">&nbsp;layer, except that it uses a simple maximum function instead of a kernel, with the </span><span class="c5">pool_size</span><span class="c7">&nbsp;parameter analogous to </span><span class="c5">kernel_size</span><span class="c7">. A </span><span class="c5">MaxPool2D</span><span class="c3 c16">&nbsp;layer doesn&#39;t have any trainable weights like a convolutional layer does in its kernel, however.</span></p><p class="c2"><span class="c3 c16"></span></p><p class="c2"><span class="c3 c16"></span></p><p class="c0"><span class="c7">Notice that after applying the ReLU function (</span><span class="c7 c19">Detect</span><span class="c7">) the feature map ends up with a lot of &quot;dead space,&quot; that is, large areas containing only 0&#39;s (the black areas in the image). Having to carry these 0 activations through the entire network would increase the size of the model without adding much useful information. Instead, we would like to </span><span class="c4">condense</span><span class="c3 c16">&nbsp;the feature map to retain only the most useful part -- the feature itself.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 403.50px; height: 415.54px;"><img alt="An example of the feature extraction process." src="images/image4.jpg" style="width: 403.50px; height: 415.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0 c13"><span class="c3 c16"></span></p><p class="c0"><span class="c7">This in fact is what </span><span class="c7 c19">maximum pooling</span><span class="c3 c16">&nbsp;does. Max pooling takes a patch of activations in the original feature map and replaces them with the maximum activation in that patch.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 361.40px; height: 208.50px;"><img alt="Maximum pooling replaces a patch with the maximum value in that patch." src="images/image1.png" style="width: 361.40px; height: 208.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3 c16">&nbsp;The pooling step increases the proportion of active pixels to zero pixels.</span></p><h1 class="c27" id="h.fw5g0shzmba5"><span class="c16 c34">Translation Invariance</span><span class="c16 c28"><a class="c6" href="https://www.google.com/url?q=https://www.kaggle.com/code/ryanholbrook/maximum-pooling%23Translation-Invariance&amp;sa=D&amp;source=editors&amp;ust=1727506785330181&amp;usg=AOvVaw1E83kmRmHTseI5PyYwHgE2">&para;</a></span></h1><p class="c0"><span class="c7">We called the zero-pixels &quot;unimportant&quot;. Does this mean they carry no information at all? In fact, the zero-pixels carry </span><span class="c4">positional information</span><span class="c7">. The blank space still positions the feature within the image. When </span><span class="c5">MaxPool2D</span><span class="c7">&nbsp;removes some of these pixels, it removes some of the positional information in the feature map. This gives a convnet a property called </span><span class="c7 c19">translation invariance</span><span class="c7">. This means that a convnet with maximum pooling will tend not to distinguish features by their </span><span class="c4">location</span><span class="c3 c16">&nbsp;in the image. (&quot;Translation&quot; is the mathematical word for changing the position of something without rotating it or changing its shape or size.)</span></p><p class="c0"><span class="c3 c16">Watch what happens when we repeatedly apply maximum pooling to the following feature map.</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 150.67px;"><img alt="Pooling tends to destroy positional information." src="images/image6.png" style="width: 624.00px; height: 150.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c7">This invariance to small differences in the positions of features is a nice property for an image classifier to have. Just because of differences in perspective or framing, the same kind of feature might be positioned in various parts of the original image, but we would still like for the classifier to recognize that they are the same. Because this invariance is </span><span class="c4">built into</span><span class="c7">&nbsp;the network, we can get away with using much less data for training: we no longer have to teach it to ignore that difference. This gives convolutional networks a big efficiency advantage over a network with only dense layers. (You&#39;ll see another way to get invariance for free in </span><span class="c7 c19">Lesson 6</span><span class="c7">&nbsp;with </span><span class="c7 c19">Data Augmentation</span><span class="c3 c16">!)</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 340.00px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 340.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c0 c16 c45" id="h.oifgtjojna05"><span class="c11 c19 c16 c30">Global Average Pooling</span></h1><p class="c0 c16"><span class="c7">We mentioned in the previous exercise that average pooling has largely been superceeded by maximum pooling within the convolutional base. There is, however, a kind of average pooling that is still widely used in the </span><span class="c4">head</span><span class="c7">&nbsp;of a convnet. This is </span><span class="c7 c19">global average pooling</span><span class="c7">. A </span><span class="c16 c24">GlobalAvgPool2D</span><span class="c7">&nbsp;layer is often used as an alternative to some or all of the hidden </span><span class="c24 c16">Dense</span><span class="c3 c16">&nbsp;layers in the head of the network, like so:</span></p><p class="c8"><span>`</span><span class="c11 c14">model = keras.Sequential([</span></p><p class="c8"><span class="c14 c11">&nbsp; &nbsp; pretrained_base,</span></p><p class="c8"><span class="c37 c38">&nbsp; &nbsp; layers.GlobalAvgPool2D(),</span></p><p class="c8"><span class="c14 c11">&nbsp; &nbsp; layers.Dense(1, activation=&#39;sigmoid&#39;)])`</span></p><h2 class="c27 c42" id="h.pffr3yq949c4"><span class="c29 c11">Strides , Padding </span></h2><p class="c8"><span class="c7">. The </span><span class="c5">strides</span><span class="c7">&nbsp;parameter says how far the window should move at each step, and the </span><span class="c5">padding</span><span class="c3 c16">&nbsp;parameter describes how we handle the pixels at the edges of the input.</span></p><p class="c2"><span class="c3 c16"></span></p><p class="c0"><span class="c12 c32">Because we want high-quality features to use for classification, convolutional layers will most often have </span><span class="c12 c35 c32">strides=(1, 1)</span><span class="c12 c32">. Increasing the stride means that we miss out on potentially valuable information in our summary. Maximum pooling layers, however, will almost always have stride values greater than 1, like </span><span class="c12 c35 c32">(2, 2)</span><span class="c12 c32">&nbsp;or </span><span class="c12 c32 c35">(3, 3)</span><span class="c3 c32">, but not larger than the window itself.</span></p><p class="c0 c13"><span class="c3 c32"></span></p><h3 class="c0 c42" id="h.5gxjrl4t4xz0"><span class="c9">Dropuout - padding - &nbsp;strides :</span></h3><p class="c2"><span class="c14 c11"></span></p><p class="c2"><span class="c37 c38"></span></p><h1 class="c27" id="h.yknjum4wy13a"><span class="c37 c34 c44">The Usefulness of Fake Data</span></h1><p class="c0"><span class="c12">The best way to improve the performance of a machine learning model is to train it on more data. The more examples the model has to learn from, the better it will be able to recognize which differences in images matter and which do not. More data helps the model to </span><span class="c12 c23">generalize</span><span class="c3">&nbsp;better.</span></p><p class="c0"><span class="c12">One easy way of getting more data is to use the data you already have. If we can transform the images in our dataset in ways that preserve the class, we can teach our classifier to ignore those kinds of transformations. For instance, whether a car is facing left or right in a photo doesn&#39;t change the fact that it is a </span><span class="c12 c23">Car</span><span class="c12">&nbsp;and not a </span><span class="c12 c23">Truck</span><span class="c12">. So, if we </span><span class="c12 c19 c32">augment</span><span class="c3">&nbsp;our training data with flipped images, our classifier will learn that &quot;left or right&quot; is a difference it should ignore.</span></p><p class="c0"><span class="c7">Data augmentation is usually done </span><span class="c4">online</span><span class="c7">, meaning, as the images are being fed into the network for training. Recall that training is usually done on mini-batches of data. This is what a batch of 16 images might look like when data augmentation is used.</span></p><p class="c8"><span class="c7">&nbsp;If you were training a </span><span class="c16 c18"><a class="c6" href="https://www.google.com/url?q=https://www.kaggle.com/c/digit-recognizer&amp;sa=D&amp;source=editors&amp;ust=1727506785333279&amp;usg=AOvVaw17Lmoj8TLPQWdhqiRhX8T9">digit recognizer</a></span><span class="c3 c16">, for instance, rotating images would mix up &#39;9&#39;s and &#39;6&#39;s. In the end, the best approach for finding good augmentations is the same as with most ML problems: try it and see!</span></p><p class="c2"><span class="c3 c16"></span></p><p class="c2"><span class="c3 c16"></span></p><p class="c8"><span class="c37 c16 c40">For data augmentation we use of some of preprocessing layers</span></p><p class="c1"><span class="c3 c16"></span></p><p class="c8"><span class="c10">model </span><span class="c17">=</span><span class="c10">&nbsp;keras</span><span class="c17">.</span><span class="c10 c11">Sequential([</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; </span><span class="c10 c23"># Preprocessing</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; preprocessing</span><span class="c17">.</span><span class="c10">RandomFlip(</span><span class="c25">&#39;horizontal&#39;</span><span class="c10">), </span><span class="c10 c23"># flip left-to-right</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; preprocessing</span><span class="c17">.</span><span class="c10">RandomContrast(</span><span class="c31">0.5</span><span class="c10">), </span><span class="c10 c23"># contrast change by up to 50%</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; </span><span class="c10 c23"># Base</span></p><p class="c8"><span class="c10 c11">&nbsp; &nbsp; pretrained_base,</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; </span><span class="c10 c23"># Head</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; layers</span><span class="c17">.</span><span class="c10 c11">Flatten(),</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; layers</span><span class="c17">.</span><span class="c10">Dense(</span><span class="c31">6</span><span class="c10">, activation</span><span class="c17">=</span><span class="c25">&#39;relu&#39;</span><span class="c10 c11">),</span></p><p class="c8"><span class="c10">&nbsp; &nbsp; layers</span><span class="c17">.</span><span class="c10">Dense(</span><span class="c31">1</span><span class="c10">, activation</span><span class="c17">=</span><span class="c25">&#39;sigmoid&#39;</span><span class="c10 c11">),</span></p><p class="c36"><span class="c10 c11">])</span></p><p class="c1"><span class="c10 c11"></span></p><p class="c36"><span class="c12 c32">Since data augmentation effectively increases the size of the dataset, we can increase the capacity of the model in turn without as much risk of overfitting.</span></p><p class="c2"><span class="c3 c16"></span></p></body></html>